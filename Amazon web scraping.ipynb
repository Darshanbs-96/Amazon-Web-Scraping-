{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7444c25a-b9d4-4659-ac0a-6e21cb6080b7",
   "metadata": {},
   "source": [
    "# Amazon Webscraping using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6575c3e8-feae-4630-88b9-6e1d092fac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import smtplib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbf4dbeb-a59a-4459-9722-2c502a823d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  Retro Computer Loading Data Graphic T-Shirt, Black, Half Sleeve\n",
      "                 \n",
      "\n",
      "                    299\n",
      "                   \n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.amazon.in/Computer-Loading-Graphic-T-Shirt-Regular/dp/B0DYF458N4/ref=sr_1_5?crid=1VSLLNG5I2TMQ&dib=eyJ2IjoiMSJ9.sF0lKgBoc4JJR0paaD9AW8awNSO9eeIu5TbF5e75gtxZoj5t7WAAJrbNWEMCNDEGmCWwWgFpqcyUJ4HJxsi8JKpVo2ifxLCnFDdoL6ccC6J6PuB27FXfTKr949_B54BhUGKeCNHpHZ8OiP3uUfn85d0QPRpTVCjiKZ08QckYC5Cr3Kic1ELgNzrcQq8PYw7MugDZCqCFPYkTDqN3rncLfE_tVlNdR3Z14Uo1GKPGZMfXjGbFdUdOLbNsmGVdTc8Gu1NuxBhcVVpHyeT-haWE93WJA25xM8Kx31UeMvyY-u0.0Nls1M6DS78AAESBKnNQy2c40Fjw_khOtCe4_AzfayY&dib_tag=se&keywords=data%2Banalyst%2Btshirt&qid=1754906124&sprefix=data%2Ba%2Caps%2C295&sr=8-5&th=1&psc=1'\n",
    "\n",
    "\n",
    "headers ={ \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0\"} \n",
    "\n",
    "page= requests.get(url, headers=headers)\n",
    "\n",
    "soup1 =  BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "soup2 =  BeautifulSoup(soup1.prettify(), \"html.parser\")\n",
    "\n",
    "#print(soup2)\n",
    "\n",
    "title = soup2.find(id='productTitle').get_text() \n",
    "\n",
    "price= soup2.find(class_='a-price-whole').get_text() \n",
    "\n",
    "\n",
    "print(title)\n",
    "print(price)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84331c06-c58b-4517-a21a-51b6a917498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retro Computer Loading Data Graphic T-Shirt, Black, Half Sleeve\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "price=price.strip()\n",
    "title=title.strip()\n",
    "\n",
    "print(title)\n",
    "print(price)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd70cd08-5be0-4f82-a995-1056ef5d4471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "print(today)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e903d49d-6e84-4abd-a86d-637caabf7413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = ['Title', 'Price', 'Date']\n",
    "data = [title, price, today]\n",
    "\n",
    "with open('AmazonwebscrapingDataset1.csv', 'w', newline='', encoding='UTF8') as f:\n",
    "    writer=csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerow(data)\n",
    "\n",
    "    \n",
    "#type(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d57b146c-13e5-4c88-b2c9-188df3275c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  Price        Date\n",
      "0  Retro Computer Loading Data Graphic T-Shirt, B...    299  2025-08-11\n",
      "1  Retro Computer Loading Data Graphic T-Shirt, B...    299  2025-08-11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\Darshan\\Desktop\\Python bcap\\AmazonwebscrapingDataset1.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90a9f32e-0550-4856-8194-1a3d62231482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are apppending\n",
    "\n",
    "with open('AmazonwebscrapingDataset1.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "    writer=csv.writer(f)\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50ec49d1-fa67-40af-9fa7-2c8c64fab2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_price():\n",
    "    url = 'https://www.amazon.in/Computer-Loading-Graphic-T-Shirt-Regular/dp/B0DYF458N4/ref=sr_1_5?crid=1VSLLNG5I2TMQ&dib=eyJ2IjoiMSJ9.sF0lKgBoc4JJR0paaD9AW8awNSO9eeIu5TbF5e75gtxZoj5t7WAAJrbNWEMCNDEGmCWwWgFpqcyUJ4HJxsi8JKpVo2ifxLCnFDdoL6ccC6J6PuB27FXfTKr949_B54BhUGKeCNHpHZ8OiP3uUfn85d0QPRpTVCjiKZ08QckYC5Cr3Kic1ELgNzrcQq8PYw7MugDZCqCFPYkTDqN3rncLfE_tVlNdR3Z14Uo1GKPGZMfXjGbFdUdOLbNsmGVdTc8Gu1NuxBhcVVpHyeT-haWE93WJA25xM8Kx31UeMvyY-u0.0Nls1M6DS78AAESBKnNQy2c40Fjw_khOtCe4_AzfayY&dib_tag=se&keywords=data%2Banalyst%2Btshirt&qid=1754906124&sprefix=data%2Ba%2Caps%2C295&sr=8-5&th=1&psc=1'\n",
    "\n",
    "\n",
    "    headers ={ \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0\"} \n",
    "\n",
    "    page= requests.get(url, headers=headers)\n",
    "\n",
    "    soup1 =  BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "    soup2 =  BeautifulSoup(soup1.prettify(), \"html.parser\")\n",
    "\n",
    "    title = soup2.find(id='productTitle').get_text() \n",
    "\n",
    "    price= soup2.find(class_='a-price-whole').get_text() \n",
    "\n",
    "    price=price.strip()\n",
    "    title=title.strip()\n",
    "\n",
    "    import datetime\n",
    "\n",
    "    today = datetime.date.today()\n",
    "\n",
    "    import csv\n",
    "\n",
    "    header = ['Title', 'Price', 'Date']\n",
    "    data = [title, price, today]\n",
    "    \n",
    "    with open('AmazonwebscrapingDataset1.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "\n",
    "   # if(price < 250):\n",
    "      #  send_mail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38a4ca68-8ba0-40a4-99c6-cfc63ca100df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m      check_price()\n\u001b[0;32m      3\u001b[0m      time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 11\u001b[0m, in \u001b[0;36mcheck_price\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m page\u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m      9\u001b[0m soup1 \u001b[38;5;241m=\u001b[39m  BeautifulSoup(page\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m soup2 \u001b[38;5;241m=\u001b[39m  BeautifulSoup(soup1\u001b[38;5;241m.\u001b[39mprettify(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m title \u001b[38;5;241m=\u001b[39m soup2\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproductTitle\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text() \n\u001b[0;32m     15\u001b[0m price\u001b[38;5;241m=\u001b[39m soup2\u001b[38;5;241m.\u001b[39mfind(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma-price-whole\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text() \n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed()\n\u001b[0;32m    336\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 478\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mfeed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarkup)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    378\u001b[0m parser\u001b[38;5;241m.\u001b[39msoup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     parser\u001b[38;5;241m.\u001b[39mfeed(markup)\n\u001b[0;32m    381\u001b[0m     parser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\html\\parser.py:129\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoahead(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\html\\parser.py:189\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m startswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m, i):\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m starttagopen\u001b[38;5;241m.\u001b[39mmatch(rawdata, i): \u001b[38;5;66;03m# < + letter\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_starttag(i)\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[0;32m    191\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_endtag(i)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\html\\parser.py:356\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_startendtag(tag, attrs)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_starttag(tag, attrs)\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCDATA_CONTENT_ELEMENTS:\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_cdata_mode(tag)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:137\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_starttag\u001b[1;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m#print(\"START\", name)\u001b[39;00m\n\u001b[0;32m    136\u001b[0m sourceline, sourcepos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetpos()\n\u001b[1;32m--> 137\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\u001b[38;5;241m.\u001b[39mhandle_starttag(\n\u001b[0;32m    138\u001b[0m     name, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, attr_dict, sourceline\u001b[38;5;241m=\u001b[39msourceline,\n\u001b[0;32m    139\u001b[0m     sourcepos\u001b[38;5;241m=\u001b[39msourcepos\n\u001b[0;32m    140\u001b[0m )\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mand\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mis_empty_element \u001b[38;5;129;01mand\u001b[39;00m handle_empty_element:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# events for tags of this name.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_endtag(name, check_already_closed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\__init__.py:749\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_starttag\u001b[1;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagStack) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    746\u001b[0m          \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only\u001b[38;5;241m.\u001b[39msearch_tag(name, attrs))):\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melement_classes\u001b[38;5;241m.\u001b[39mget(Tag, Tag)(\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder, name, namespace, nsprefix, attrs,\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentTag, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_most_recent_element,\n\u001b[0;32m    752\u001b[0m     sourceline\u001b[38;5;241m=\u001b[39msourceline, sourcepos\u001b[38;5;241m=\u001b[39msourcepos,\n\u001b[0;32m    753\u001b[0m     namespaces\u001b[38;5;241m=\u001b[39mnamespaces\n\u001b[0;32m    754\u001b[0m )\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tag\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\element.py:1293\u001b[0m, in \u001b[0;36mTag.__init__\u001b[1;34m(self, parser, builder, name, namespace, prefix, attrs, parent, previous, is_xml, sourceline, sourcepos, can_be_empty_element, cdata_list_attributes, preserve_whitespace_tags, interesting_string_types, namespaces)\u001b[0m\n\u001b[0;32m   1290\u001b[0m builder\u001b[38;5;241m.\u001b[39mset_up_substitutions(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;66;03m# Ask the TreeBuilder whether this tag might be an empty-element tag.\u001b[39;00m\n\u001b[1;32m-> 1293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcan_be_empty_element \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mcan_be_empty_element(name)\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;66;03m# Keep track of the list of attributes of this tag that\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;66;03m# might need to be treated as a list.\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;66;03m# (unlike can_be_empty_element), we almost never need\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;66;03m# to check this.\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcdata_list_attributes \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mcdata_list_attributes\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\builder\\__init__.py:205\u001b[0m, in \u001b[0;36mTreeBuilder.can_be_empty_element\u001b[1;34m(self, tag_name)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Do any work necessary to reset the underlying parser\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    for a new document.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    By default, this does nothing.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcan_be_empty_element\u001b[39m(\u001b[38;5;28mself\u001b[39m, tag_name):\n\u001b[0;32m    206\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Might a tag with this name be an empty-element tag?\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m    The final markup may or may not actually present this tag as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    :param tag_name: The name of a markup tag.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mempty_element_tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "     check_price()\n",
    "     time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ca07673-3c62-455e-8b6d-a2ecb8a18b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  Price        Date\n",
      "0  Retro Computer Loading Data Graphic T-Shirt, B...    299  2025-08-11\n",
      "1  Retro Computer Loading Data Graphic T-Shirt, B...    299  2025-08-11\n",
      "2  Retro Computer Loading Data Graphic T-Shirt, B...    299  2025-08-11\n",
      "3  Retro Computer Loading Data Graphic T-Shirt, B...    299  2025-08-11\n",
      "4  Retro Computer Loading Data Graphic T-Shirt, B...    299  2025-08-11\n",
      "5  Retro Computer Loading Data Graphic T-Shirt, B...    299  2025-08-11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\Darshan\\Desktop\\Python bcap\\AmazonwebscrapingDataset1.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b704c0eb-f69f-4bf9-8ccf-70fb593fa657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_mail():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
